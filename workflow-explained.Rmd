# Feather River EDI upload workflow 

The data format acknowledges the constraints and realities of field data collection and the data management unique to Feather River, while also ensuring data are interoperable across other datasets used in the Feather River surveys.

Establishing an agreed upon data format is an important component of providing timely and automated access to data for Feather River 

This repository cleans Fether River monitoring data:

## Data pull and checks

Workflow outsources two main scripts:

  - [**early_snorkel_db_pull.R**](https://github.com/FlowWest/edi-feather-snorkel/blob/edi-workflow-documentation/data-raw/data-prep-scripts/early_snorkel_db_pull.R): pulls data from early historical snorkel database
    - Pulls data and cleans it. Inspects the structure of early snorkel observations and metadata, ensuring that all columns are formatted as expected
    - a unit lookup table (`sampling_unit_lookup`) is created to help joining sampling units to river mile, section and channel type
    
  - [**revised_snorkel_db_pull.R**](https://github.com/FlowWest/edi-feather-snorkel/tree/main/data-raw/data-prep-scripts/revised_snorkel_db_pull.R): pulls more recent snorkel data, representing revised version
    - Similar to first script, data checks are done to verify the integrity of the current observations and metadata
    - Column inconsistencies are identified. 

  - [**combine_and_clean_data.R**](https://github.com/FlowWest/edi-feather-snorkel/tree/main/data-raw/data-prep-scripts/combine_and_clean_data.R): Script consolidates the cleaned data from previous two scripts and transforms data to ensure consistency and accuracy across datasets
    - early and current snorkel data metadata are combines (`combined_snorkel_metadata`)
    - duplicates and missing data are addressed by filtering out or categorizing properly
      - cleaned metadata is saved as csv `survey_characteristics.csv` 
    - *early snorkel observations* and current observations are merged to create `combined_snorkel_observations`
    - Rows with messy unit identifiers are filtered out, and missing counts are replaced with zeros
      - cleaned observations is saved as csv `fish_observations.csv`
    - *lookup table* is created that combines information about river miles and survey locations
      - table is saved as csv `locations_lookup.csv`
    
    
  
  
### Summary of CSV Generation for EDI Upload: 

  - [fish_observations.csv](https://github.com/FlowWest/edi-feather-snorkel/blob/main/data/fish_observations.csv): Generated from combined_snorkel_observations in combine_and_clean_data.R.
  - [survey_characteristics.csv](https://github.com/FlowWest/edi-feather-snorkel/blob/main/data/fish_observations.csv): Created from combined_snorkel_metadata in combine_and_clean_data.R.
  - [locations_lookup.csv](https://github.com/FlowWest/edi-feather-snorkel/blob/main/data/locations_lookup.csv): Generated from full_location_lookup in combine_and_clean_data.R.
  
  
**Conclusion**

This workflow efficiently pulls, cleans, and combines snorkel survey data from historical and current sources, ensuring that clean and consistent datasets are saved for future analysis. Once data data tables are processed and combined as necessary, they are written onto the [**data folder**](data) to start uploading process
  
  
## EDI make_metadata/upload

- placeholder for make_metadata workflow explanation
